{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3971,"databundleVersionId":32703,"sourceType":"competition"}],"dockerImageVersionId":30615,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import zipfile\nimport os\nimport pandas as pd\n# Define the path to  ZIP file\nzip_file_paths = ['/kaggle/input/word2vec-nlp-tutorial/labeledTrainData.tsv.zip', '/kaggle/input/word2vec-nlp-tutorial/testData.tsv.zip']\n\n# Define the extraction directory\nextraction_dir = '/kaggle/working/extracted'\n\n# Create the extraction directory if it doesn't exist\nos.makedirs(extraction_dir, exist_ok=True)\n\n# Extract the ZIP file\nfor i in zip_file_paths:\n    with zipfile.ZipFile(i, 'r') as zip_ref:\n        zip_ref.extractall(extraction_dir)\n\n# List the contents of the extraction directory\nextracted_files = os.listdir(extraction_dir)\nprint(\"Extracted files:\", extracted_files)\n\n# Read the data\ntrain = pd.read_csv('/kaggle/working/extracted/labeledTrainData.tsv', delimiter='\\t')\ntest = pd.read_csv('/kaggle/working/extracted/testData.tsv', delimiter='\\t')","metadata":{"_uuid":"9e9c703c-1a1b-435e-942c-2c55b3011ddc","_cell_guid":"adc81c55-73b4-4643-85cb-f6eb0fb9bb2c","collapsed":false,"execution":{"iopub.status.busy":"2023-12-28T14:51:19.366307Z","iopub.execute_input":"2023-12-28T14:51:19.366627Z","iopub.status.idle":"2023-12-28T14:51:21.526615Z","shell.execute_reply.started":"2023-12-28T14:51:19.366598Z","shell.execute_reply":"2023-12-28T14:51:21.525429Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from bs4 import BeautifulSoup\nimport re\nfrom nltk.corpus import stopwords\nfrom sklearn.feature_extraction.text import CountVectorizer\n# Function to clean the text\ndef clean_text(raw_text):\n    # Remove HTML markup\n    text_no_html = BeautifulSoup(raw_text, 'html.parser').get_text()\n    \n    # Remove non-letters, keep only letters\n    letters_only = re.sub(\"[^a-zA-Z]\", \" \", text_no_html)\n    \n    # Convert to lowercase and split into words\n    words = letters_only.lower().split()\n    \n    # Convert stopwords to a set for faster processing\n    stops = set(stopwords.words(\"english\"))\n    \n    # Remove stopwords\n    meaningful_words = [w for w in words if not w in stops]\n    \n    # Join the words back into one string separated by space\n    return \" \".join(meaningful_words)\n\n# Apply cleaning to the training set\ntrain['clean_review'] = train['review'].apply(clean_text)\n\n# Initialize the CountVectorizer\nvectorizer = CountVectorizer(analyzer='word', tokenizer=None, preprocessor=None, stop_words=None, max_features=5000, ngram_range=(1, 2))\n\n# Fit and transform the vectorizer to the training data\ntrain_data_features = vectorizer.fit_transform(train['clean_review'])\n\n# Convert the result to an array\ntrain_data_features = train_data_features.toarray()","metadata":{"_uuid":"33f79d9c-53b0-49cf-9066-ab525fb9b69d","_cell_guid":"05cdc512-7fd5-493a-8a61-06fd73dadd21","collapsed":false,"execution":{"iopub.status.busy":"2023-12-28T14:51:21.528803Z","iopub.execute_input":"2023-12-28T14:51:21.529487Z","iopub.status.idle":"2023-12-28T14:51:57.710399Z","shell.execute_reply.started":"2023-12-28T14:51:21.529429Z","shell.execute_reply":"2023-12-28T14:51:57.709480Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# VotingClassifier with **Random Forest** and **XGBClassifier**","metadata":{"_uuid":"25eae6b8-6553-4de2-913f-ed1767c3bc82","_cell_guid":"3c603cc1-fa00-4bce-bb67-1ccf12120c16","trusted":true}},{"cell_type":"code","source":"\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import VotingClassifier\n\n\n\n\n# Initialize models\nrandom_forest = RandomForestClassifier(n_estimators=100)\nxgb = XGBClassifier()\n\n# Create a Voting Classifier with RandomForest and XGBoost\nvoting_classifier = VotingClassifier(estimators=[('rf', random_forest), ('xgb', xgb)], voting='soft')\n\n# Train the Voting Classifier on the training data\nvoting_classifier.fit(train_data_features, train['sentiment'])\n\n# Apply the same cleaning to the test set\ntest['clean_review'] = test['review'].apply(clean_text)\n\n# Transform the test data\ntest_data_features = vectorizer.transform(test['clean_review'])\ntest_data_features = test_data_features.toarray()\n\n# Predict sentiment for the test set\nresult = voting_classifier.predict(test_data_features)","metadata":{"_uuid":"84e6533c-1e67-4721-a811-e251c1901b12","_cell_guid":"665b16bb-3b9a-4d8a-8f37-fca3bc99039b","collapsed":false,"execution":{"iopub.status.busy":"2023-12-28T14:51:57.711746Z","iopub.execute_input":"2023-12-28T14:51:57.712788Z","iopub.status.idle":"2023-12-28T14:53:48.782793Z","shell.execute_reply.started":"2023-12-28T14:51:57.712755Z","shell.execute_reply":"2023-12-28T14:53:48.781715Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"_uuid":"c55278ae-be33-44e7-854b-b2171297ccee","_cell_guid":"98576313-f951-40d1-8e78-577ebd59d94a","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict_sentiment(sentence, model, vectorizer):\n    cleaned_sentence = clean_text(sentence) \n    # Transform the cleaned sentence using the vectorizer\n    sequence = vectorizer.transform([cleaned_sentence])\n    prediction = model.predict(sequence)[0]\n    sentiment = \"Positive\" if prediction == 1 else \"Negative\"\n    return sentiment, sentence\n# Function to visualize predictions\ndef visualize_predictions(sentences, model, vectorizer):\n    for sentence in sentences:\n        sentiment, original_sentence = predict_sentiment(sentence, model, vectorizer)\n        print(f\"Review: {original_sentence} | Predicted Sentiment: {sentiment}\")\n        print(\"--------------------------------------------------\")","metadata":{"_uuid":"15006a0c-2610-432c-be50-33a1f9a00ce7","_cell_guid":"f7026466-0af9-4663-8ee8-6ae603a514ab","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Example sentences for visualization\nexample_sentences = [\n    \"This movie was fantastic, I loved it!\",\n    \"This movie is a disaster,it is so awful that once you know what is coming,Look away and spend your time on more meaningful content.\",\n    \"The performance was average, but the story was captivating.\"\n]\n# Visualize predictions for the example sentences\nvisualize_predictions(example_sentences, voting_classifier, vectorizer)","metadata":{"_uuid":"24abca54-6f7d-4ed1-8fda-488d79e422c5","_cell_guid":"f2e14a51-83fa-414f-867c-c177b8aaaf48","collapsed":false,"execution":{"iopub.status.busy":"2023-12-28T15:41:16.147373Z","iopub.execute_input":"2023-12-28T15:41:16.147903Z","iopub.status.idle":"2023-12-28T15:41:16.200929Z","shell.execute_reply.started":"2023-12-28T15:41:16.147867Z","shell.execute_reply":"2023-12-28T15:41:16.199579Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# Create a submission file (86% test accuracy)\nsubmission = pd.DataFrame({'id': test['id'], 'sentiment': result})\nsubmission.to_csv('submission.csv', index=False)","metadata":{"_uuid":"5d6aec14-66ab-4e71-8cc1-b57fecbfceb6","_cell_guid":"59034a60-a0d8-4704-8165-79d53aa1ae27","collapsed":false,"execution":{"iopub.status.busy":"2023-12-15T19:59:32.822912Z","iopub.execute_input":"2023-12-15T19:59:32.823366Z","iopub.status.idle":"2023-12-15T19:59:32.879843Z","shell.execute_reply.started":"2023-12-15T19:59:32.823339Z","shell.execute_reply":"2023-12-15T19:59:32.879138Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **LSTM**","metadata":{"_uuid":"21377f19-7b4d-4641-9e03-ff69ff3e01b4","_cell_guid":"9f1acec5-b0bb-4367-a103-03b1e6ece6d1","trusted":true}},{"cell_type":"code","source":"from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Embedding, LSTM, Dense, SpatialDropout1D,Bidirectional\nfrom sklearn.model_selection import train_test_split\n\n\n\n# Tokenizing text and converting into sequences\ntokenizer = Tokenizer(num_words=5000, lower=True)\ntokenizer.fit_on_texts(train['clean_review'])\nX = tokenizer.texts_to_sequences(train['clean_review'])\nX = pad_sequences(X, maxlen=200)\n\n# Splitting data into train and validation sets\nX_train, X_val, y_train, y_val = train_test_split(X, train['sentiment'], test_size=0.2, random_state=42)\n\n# Model architecture\nmodel = Sequential()\nmodel.add(Embedding(input_dim=5000, output_dim=128, input_length=200))\nmodel.add(SpatialDropout1D(0.2))\nmodel.add(LSTM(512, dropout=0.2, recurrent_dropout=0.2, return_sequences=True))\nmodel.add(Bidirectional(LSTM(256, dropout=0.2, recurrent_dropout=0.2, return_sequences=True)))\nmodel.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\nmodel.add(Dense(1, activation='sigmoid'))\n\n# Compile the model\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\n# Train the model\nhistory = model.fit(X_train, y_train, epochs=10, batch_size=64, validation_data=(X_val, y_val), verbose=1)","metadata":{"_uuid":"36ab0c32-4f57-46d2-b3b8-6bc33b3c0650","_cell_guid":"52115d4a-836c-4fde-a156-47f38f4f0208","collapsed":false,"execution":{"iopub.status.busy":"2023-12-15T19:59:32.880843Z","iopub.execute_input":"2023-12-15T19:59:32.881132Z","iopub.status.idle":"2023-12-15T22:16:50.304581Z","shell.execute_reply.started":"2023-12-15T19:59:32.881088Z","shell.execute_reply":"2023-12-15T22:16:50.303757Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Visualize training and validation accuracy\nplt.figure(figsize=(8, 6))\nplt.plot(history.history['accuracy'], label='Training Accuracy')\nplt.plot(history.history['val_accuracy'], label='Validation Accuracy')\nplt.title('Training and Validation Accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()\n\n# Visualize training and validation loss\nplt.figure(figsize=(8, 6))\nplt.plot(history.history['loss'], label='Training Loss')\nplt.plot(history.history['val_loss'], label='Validation Loss')\nplt.title('Training and Validation Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()","metadata":{"_uuid":"f5122c46-81d8-44f9-b645-bed9e5cd5bf8","_cell_guid":"7b7ef3d5-10b9-4375-8da7-f1253389054b","collapsed":false,"execution":{"iopub.status.busy":"2023-12-15T22:16:50.307639Z","iopub.execute_input":"2023-12-15T22:16:50.307910Z","iopub.status.idle":"2023-12-15T22:16:50.885827Z","shell.execute_reply.started":"2023-12-15T22:16:50.307886Z","shell.execute_reply":"2023-12-15T22:16:50.884919Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Preprocess the test data\nX_test = tokenizer.texts_to_sequences(test['clean_review'])\nX_test = pad_sequences(X_test, maxlen=200)\n\n# Predict sentiment for the test set\nresult_dl = model.predict(X_test)","metadata":{"_uuid":"08e54966-3007-48f2-9985-6ea92dc26124","_cell_guid":"3daf3ec6-9d7a-4ccc-9d13-4159ea1c2a09","collapsed":false,"execution":{"iopub.status.busy":"2023-12-15T22:16:50.887146Z","iopub.execute_input":"2023-12-15T22:16:50.887799Z","iopub.status.idle":"2023-12-15T22:19:12.304203Z","shell.execute_reply.started":"2023-12-15T22:16:50.887762Z","shell.execute_reply":"2023-12-15T22:19:12.303296Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a submission file (92% test accuracy)\nsubmission = pd.DataFrame({'id': test['id'], 'sentiment': result_dl.flatten()})\nsubmission.to_csv('submission.csv', index=False)","metadata":{"_uuid":"d8dffa3c-ef00-4809-b394-9b004ed7f088","_cell_guid":"a92ad7ba-2857-4c38-b88d-ba575c32075f","collapsed":false,"execution":{"iopub.status.busy":"2023-12-15T22:19:12.305512Z","iopub.execute_input":"2023-12-15T22:19:12.305804Z","iopub.status.idle":"2023-12-15T22:19:12.395359Z","shell.execute_reply.started":"2023-12-15T22:19:12.305777Z","shell.execute_reply":"2023-12-15T22:19:12.394480Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"_uuid":"d8a2d618-21ff-461c-ba0d-28b6e1034cdb","_cell_guid":"b6ce2413-a4c3-4886-a680-4a242eb86bf2","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]}]}